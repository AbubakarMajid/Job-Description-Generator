{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import streamlit as st\n",
    "import langchain\n",
    "from langchain.llms import OpenAI\n",
    "import os\n",
    "import re\n",
    "\n",
    "# api key\n",
    "os.environ['OPENAI_API_KEY'] = \"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\"\n",
    "llm = OpenAI(temperature=0, model=\"text-davinci-003\", max_tokens=500)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "role_title = \"Data Scientist\"\n",
    "\n",
    "job_description = \"You should be able to understand and work with complex data sets. Additionally, you should be able to use statistical software packages and be familiar with programming languages such as Python or R. Data scientists also typically have a certification from an accredited program.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "question_list = f\"\"\"Can you provide a high-level overview of the {role_title} role and its importance to the company?\n",
    "What are the top 3-5 critical responsibilities for this role?\n",
    "Does this role involve managing others? If so please provide some details (e.g. number of people reporting to this role, etc.)\n",
    "What technical skills, such as proficiency in programming languages, data analysis tools, or ML frameworks, are crucial for this role?\n",
    "What educational background or certifications are essential or desirable for this position?\n",
    "Aside from technical skills, what other skills or experience are significant for this role?\n",
    "What will be the key performance indicators for this role? How will their success be measured?\n",
    "Can you describe a typical workday for the {role_title}?\n",
    "What specific software, tools, or equipment should the candidate be proficient with?\n",
    "What types of projects will the {role_title} typically be involved in?\n",
    "How does this role contribute to the overall objectives of the department and the company?\n",
    "To whom will the {role_title} be reporting? How does the team structure look like?\n",
    "Who will the {role_title} be collaborating with frequently?\n",
    "How much knowledge of our industry is required for this role?\n",
    "Has this role been filled previously? If yes, what were the strengths and weaknesses of the person who previously filled this role?\n",
    "What interpersonal or soft skills are crucial for this role?\n",
    "What type of individual would best fit within the team and company culture?\n",
    "What potential growth opportunities exist for this role within the company?\n",
    "What are your expectations for this role in the first 30, 60, and 90 days?\n",
    "What is the budget for this role? What is the expected salary range?\n",
    "What will the selection and interview process look like for this role?\n",
    "For each of the skills and experiences we've discussed, could you specify which are 'must-haves' and which are 'nice-to-haves'?\n",
    "Can you describe the company's culture and values? What makes the company stand out as an employer?\n",
    "What are the key benefits and perks associated with this role that we can highlight to potential candidates?\n",
    "Are there any significant, high-impact projects that the {role_title} will get to work on? This can often be a significant draw for candidates.\n",
    "Can you give us the email or link where people can apply for the job? Any application deadline?\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: You exceeded your current quota, please check your plan and billing details..\n",
      "Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: You exceeded your current quota, please check your plan and billing details..\n",
      "Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: You exceeded your current quota, please check your plan and billing details..\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m llm\u001b[39m.\u001b[39;49mpredict(\u001b[39mf\u001b[39;49m\u001b[39m\"\"\"\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mHere is the Job Description:\u001b[39;49m\u001b[39m\\n\u001b[39;49;00m\u001b[39m{\u001b[39;49;00mjob_description\u001b[39m}\u001b[39;49;00m\u001b[39m\\n\u001b[39;49;00m\u001b[39mHere are the list of questions\u001b[39;49m\u001b[39m\\n\u001b[39;49;00m\u001b[39m{\u001b[39;49;00mquestion_list\u001b[39m}\u001b[39;49;00m\u001b[39m\\n\u001b[39;49;00m\u001b[39m\\n\u001b[39;49;00m\u001b[39mAfter thoroughly reading the job description, Make a breakdown that which questions are not answered in the job description.\u001b[39;49m\u001b[39m\"\"\"\u001b[39;49m)\n",
      "File \u001b[1;32md:\\ana\\envs\\jobD\\Lib\\site-packages\\langchain\\llms\\base.py:842\u001b[0m, in \u001b[0;36mBaseLLM.predict\u001b[1;34m(self, text, stop, **kwargs)\u001b[0m\n\u001b[0;32m    840\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    841\u001b[0m     _stop \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(stop)\n\u001b[1;32m--> 842\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m(text, stop\u001b[39m=\u001b[39;49m_stop, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32md:\\ana\\envs\\jobD\\Lib\\site-packages\\langchain\\llms\\base.py:802\u001b[0m, in \u001b[0;36mBaseLLM.__call__\u001b[1;34m(self, prompt, stop, callbacks, tags, metadata, **kwargs)\u001b[0m\n\u001b[0;32m    795\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(prompt, \u001b[39mstr\u001b[39m):\n\u001b[0;32m    796\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    797\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mArgument `prompt` is expected to be a string. Instead found \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    798\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mtype\u001b[39m(prompt)\u001b[39m}\u001b[39;00m\u001b[39m. If you want to run the LLM on multiple prompts, use \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    799\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m`generate` instead.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    800\u001b[0m     )\n\u001b[0;32m    801\u001b[0m \u001b[39mreturn\u001b[39;00m (\n\u001b[1;32m--> 802\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgenerate(\n\u001b[0;32m    803\u001b[0m         [prompt],\n\u001b[0;32m    804\u001b[0m         stop\u001b[39m=\u001b[39;49mstop,\n\u001b[0;32m    805\u001b[0m         callbacks\u001b[39m=\u001b[39;49mcallbacks,\n\u001b[0;32m    806\u001b[0m         tags\u001b[39m=\u001b[39;49mtags,\n\u001b[0;32m    807\u001b[0m         metadata\u001b[39m=\u001b[39;49mmetadata,\n\u001b[0;32m    808\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[0;32m    809\u001b[0m     )\n\u001b[0;32m    810\u001b[0m     \u001b[39m.\u001b[39mgenerations[\u001b[39m0\u001b[39m][\u001b[39m0\u001b[39m]\n\u001b[0;32m    811\u001b[0m     \u001b[39m.\u001b[39mtext\n\u001b[0;32m    812\u001b[0m )\n",
      "File \u001b[1;32md:\\ana\\envs\\jobD\\Lib\\site-packages\\langchain\\llms\\base.py:598\u001b[0m, in \u001b[0;36mBaseLLM.generate\u001b[1;34m(self, prompts, stop, callbacks, tags, metadata, **kwargs)\u001b[0m\n\u001b[0;32m    589\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    590\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mAsked to cache, but no cache found at `langchain.cache`.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    591\u001b[0m         )\n\u001b[0;32m    592\u001b[0m     run_managers \u001b[39m=\u001b[39m [\n\u001b[0;32m    593\u001b[0m         callback_manager\u001b[39m.\u001b[39mon_llm_start(\n\u001b[0;32m    594\u001b[0m             dumpd(\u001b[39mself\u001b[39m), [prompt], invocation_params\u001b[39m=\u001b[39mparams, options\u001b[39m=\u001b[39moptions\n\u001b[0;32m    595\u001b[0m         )[\u001b[39m0\u001b[39m]\n\u001b[0;32m    596\u001b[0m         \u001b[39mfor\u001b[39;00m callback_manager, prompt \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(callback_managers, prompts)\n\u001b[0;32m    597\u001b[0m     ]\n\u001b[1;32m--> 598\u001b[0m     output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_generate_helper(\n\u001b[0;32m    599\u001b[0m         prompts, stop, run_managers, \u001b[39mbool\u001b[39;49m(new_arg_supported), \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs\n\u001b[0;32m    600\u001b[0m     )\n\u001b[0;32m    601\u001b[0m     \u001b[39mreturn\u001b[39;00m output\n\u001b[0;32m    602\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(missing_prompts) \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n",
      "File \u001b[1;32md:\\ana\\envs\\jobD\\Lib\\site-packages\\langchain\\llms\\base.py:504\u001b[0m, in \u001b[0;36mBaseLLM._generate_helper\u001b[1;34m(self, prompts, stop, run_managers, new_arg_supported, **kwargs)\u001b[0m\n\u001b[0;32m    502\u001b[0m     \u001b[39mfor\u001b[39;00m run_manager \u001b[39min\u001b[39;00m run_managers:\n\u001b[0;32m    503\u001b[0m         run_manager\u001b[39m.\u001b[39mon_llm_error(e)\n\u001b[1;32m--> 504\u001b[0m     \u001b[39mraise\u001b[39;00m e\n\u001b[0;32m    505\u001b[0m flattened_outputs \u001b[39m=\u001b[39m output\u001b[39m.\u001b[39mflatten()\n\u001b[0;32m    506\u001b[0m \u001b[39mfor\u001b[39;00m manager, flattened_output \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(run_managers, flattened_outputs):\n",
      "File \u001b[1;32md:\\ana\\envs\\jobD\\Lib\\site-packages\\langchain\\llms\\base.py:491\u001b[0m, in \u001b[0;36mBaseLLM._generate_helper\u001b[1;34m(self, prompts, stop, run_managers, new_arg_supported, **kwargs)\u001b[0m\n\u001b[0;32m    481\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_generate_helper\u001b[39m(\n\u001b[0;32m    482\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m    483\u001b[0m     prompts: List[\u001b[39mstr\u001b[39m],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    487\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: Any,\n\u001b[0;32m    488\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m LLMResult:\n\u001b[0;32m    489\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m    490\u001b[0m         output \u001b[39m=\u001b[39m (\n\u001b[1;32m--> 491\u001b[0m             \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_generate(\n\u001b[0;32m    492\u001b[0m                 prompts,\n\u001b[0;32m    493\u001b[0m                 stop\u001b[39m=\u001b[39;49mstop,\n\u001b[0;32m    494\u001b[0m                 \u001b[39m# TODO: support multiple run managers\u001b[39;49;00m\n\u001b[0;32m    495\u001b[0m                 run_manager\u001b[39m=\u001b[39;49mrun_managers[\u001b[39m0\u001b[39;49m] \u001b[39mif\u001b[39;49;00m run_managers \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m,\n\u001b[0;32m    496\u001b[0m                 \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[0;32m    497\u001b[0m             )\n\u001b[0;32m    498\u001b[0m             \u001b[39mif\u001b[39;00m new_arg_supported\n\u001b[0;32m    499\u001b[0m             \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_generate(prompts, stop\u001b[39m=\u001b[39mstop)\n\u001b[0;32m    500\u001b[0m         )\n\u001b[0;32m    501\u001b[0m     \u001b[39mexcept\u001b[39;00m (\u001b[39mKeyboardInterrupt\u001b[39;00m, \u001b[39mException\u001b[39;00m) \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    502\u001b[0m         \u001b[39mfor\u001b[39;00m run_manager \u001b[39min\u001b[39;00m run_managers:\n",
      "File \u001b[1;32md:\\ana\\envs\\jobD\\Lib\\site-packages\\langchain\\llms\\openai.py:384\u001b[0m, in \u001b[0;36mBaseOpenAI._generate\u001b[1;34m(self, prompts, stop, run_manager, **kwargs)\u001b[0m\n\u001b[0;32m    372\u001b[0m     choices\u001b[39m.\u001b[39mappend(\n\u001b[0;32m    373\u001b[0m         {\n\u001b[0;32m    374\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mtext\u001b[39m\u001b[39m\"\u001b[39m: generation\u001b[39m.\u001b[39mtext,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    381\u001b[0m         }\n\u001b[0;32m    382\u001b[0m     )\n\u001b[0;32m    383\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 384\u001b[0m     response \u001b[39m=\u001b[39m completion_with_retry(\n\u001b[0;32m    385\u001b[0m         \u001b[39mself\u001b[39;49m, prompt\u001b[39m=\u001b[39;49m_prompts, run_manager\u001b[39m=\u001b[39;49mrun_manager, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mparams\n\u001b[0;32m    386\u001b[0m     )\n\u001b[0;32m    387\u001b[0m     choices\u001b[39m.\u001b[39mextend(response[\u001b[39m\"\u001b[39m\u001b[39mchoices\u001b[39m\u001b[39m\"\u001b[39m])\n\u001b[0;32m    388\u001b[0m     update_token_usage(_keys, response, token_usage)\n",
      "File \u001b[1;32md:\\ana\\envs\\jobD\\Lib\\site-packages\\langchain\\llms\\openai.py:116\u001b[0m, in \u001b[0;36mcompletion_with_retry\u001b[1;34m(llm, run_manager, **kwargs)\u001b[0m\n\u001b[0;32m    112\u001b[0m \u001b[39m@retry_decorator\u001b[39m\n\u001b[0;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_completion_with_retry\u001b[39m(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: Any) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Any:\n\u001b[0;32m    114\u001b[0m     \u001b[39mreturn\u001b[39;00m llm\u001b[39m.\u001b[39mclient\u001b[39m.\u001b[39mcreate(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m--> 116\u001b[0m \u001b[39mreturn\u001b[39;00m _completion_with_retry(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32md:\\ana\\envs\\jobD\\Lib\\site-packages\\tenacity\\__init__.py:289\u001b[0m, in \u001b[0;36mBaseRetrying.wraps.<locals>.wrapped_f\u001b[1;34m(*args, **kw)\u001b[0m\n\u001b[0;32m    287\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(f)\n\u001b[0;32m    288\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwrapped_f\u001b[39m(\u001b[39m*\u001b[39margs: t\u001b[39m.\u001b[39mAny, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkw: t\u001b[39m.\u001b[39mAny) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m t\u001b[39m.\u001b[39mAny:\n\u001b[1;32m--> 289\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m(f, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkw)\n",
      "File \u001b[1;32md:\\ana\\envs\\jobD\\Lib\\site-packages\\tenacity\\__init__.py:389\u001b[0m, in \u001b[0;36mRetrying.__call__\u001b[1;34m(self, fn, *args, **kwargs)\u001b[0m\n\u001b[0;32m    387\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(do, DoSleep):\n\u001b[0;32m    388\u001b[0m     retry_state\u001b[39m.\u001b[39mprepare_for_next_attempt()\n\u001b[1;32m--> 389\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msleep(do)\n\u001b[0;32m    390\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    391\u001b[0m     \u001b[39mreturn\u001b[39;00m do\n",
      "File \u001b[1;32md:\\ana\\envs\\jobD\\Lib\\site-packages\\tenacity\\nap.py:31\u001b[0m, in \u001b[0;36msleep\u001b[1;34m(seconds)\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39msleep\u001b[39m(seconds: \u001b[39mfloat\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m     26\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m     27\u001b[0m \u001b[39m    Sleep strategy that delays execution for a given number of seconds.\u001b[39;00m\n\u001b[0;32m     28\u001b[0m \n\u001b[0;32m     29\u001b[0m \u001b[39m    This is the default strategy, and may be mocked out for unit testing.\u001b[39;00m\n\u001b[0;32m     30\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 31\u001b[0m     time\u001b[39m.\u001b[39;49msleep(seconds)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "llm.predict(f\"\"\"\"Here is the Job Description:\\n{job_description}\\nHere are the list of questions\\n{question_list}\\n\\nAfter thoroughly reading the job description, Make a breakdown that which questions are not answered in the job description.\"\"\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jobD",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
